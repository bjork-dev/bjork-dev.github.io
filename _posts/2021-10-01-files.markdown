---
author: Liam Bj√∂rkman
tags: cloud files blobs azure storage
---

To demonstrate how how to implement azure storage in a .NET Core Web App, I've created a sample web app that can upload, display and download images from a public blob container. 

<img src="/img/website1.png">

To help me get started and save a little bit of time I've used the [Microsoft Learn Store Data In Azure](https://github.com/MicrosoftDocs/mslearn-store-data-in-azure) project as template and modified it to match my requirements.



Diagram of data flow in the application

<img src="/img/dig.png">

The diagram describes the following

* BlobApp - The Web App that presents the data.
* FilesController - API Controller that is responsible for getting/posting against our blob storage by using the `Azure.Storage.Blobs` library.
* Azure - Our endpoint is in our Azure Storage Account, where we can point to the specified storage type and container name.



## Code

Since I'm only working with one container, named **`public`** and have set it to public access level, some variables are hardcoded in the following snippets. The `connection string` for the storage account is saved in the app service's `appsettings`, and is not included in source control (saved in `secrets.json` in local environment). If we would want to limit the access level to our files we could either use an access key secret that could be stored and retrieved from `Azure KeyVault` or `appsettings`, or if we want to allow for third-party support (like an external service to upload and download to our container) we could implement SAS (Shared Access Signature).

**SAS -** A shared access signature is a URI that grants restricted access rights to Azure Storage resources. You can provide a shared access signature to clients who should not be trusted with your storage account key but whom you wish to delegate access to certain storage account resources. By distributing a shared access signature URI to these clients, you grant them access to a resource for a specified period of time.

### Backend Part

### BlobStorage

Class that contains the methods used to communicate with Azure. 

**GetNames** - Gets all blob names from Azure and returns them in a `IEnumerable<string>`.

```c#
public async Task<IEnumerable<string>> GetNames()
        {
            List<string> names = new List<string>();

            BlobServiceClient blobServiceClient = new BlobServiceClient(connection);

            // Get the container the blobs are saved in
            BlobContainerClient containerClient = blobServiceClient.GetBlobContainerClient("public");

            // This gets the info about the blobs in the container
            AsyncPageable<BlobItem> blobs = containerClient.GetBlobsAsync();

            await foreach (var blob in blobs)
            {
                names.Add(blob.Name);
            }
            return names;
        }
```

**Save** - Uploads the image in steam format to the container.

```c#
  public Task Save(Stream fileStream, string name)
        {
            BlobServiceClient blobServiceClient = new BlobServiceClient(connection);
            
            // Get the container (folder) the file will be saved in
            BlobContainerClient containerClient = blobServiceClient.GetBlobContainerClient("public");

            // Get the Blob Client used to interact with (including create) the blob
            BlobClient blobClient = containerClient.GetBlobClient(name);

            // Upload the blob
            return blobClient.UploadAsync(fileStream);
        }
```

**Load** - Opens a stream to download blobs from.

```c#
public Task<Stream> Load(string name)
        {
            BlobServiceClient blobServiceClient = new BlobServiceClient(connection);

            // Get the container the blobs are saved in
            BlobContainerClient containerClient = blobServiceClient.GetBlobContainerClient("public");

            // Get a client to operate on the blob so we can read it.
            BlobClient blobClient = containerClient.GetBlobClient(name);
            
            return blobClient.OpenReadAsync();
        }
```



### FilesController

Our controller implements the `BlobStorage` class and executes it in HTTP requests.

**Index** - Called by the page on load. Returns the name of all blobs from our `BlobStorage` class. The **`baseUrl`** is assigned to each string allowing us to view & download the file.

```c#
[HttpGet]
        public async Task<IActionResult> Index()
        {
            var names = await storage.GetNames();

            var baseUrl = "https://bjorkdev.blob.core.windows.net/public";

            var urls = names.Select(n => $"{baseUrl}/{n}");

            return Ok(urls);
        }
```

**Upload** - Called on post, takes in our file in `IFormFile` format and saves it to our blob container as a byte stream.

**IFormFile -** Represents a file sent with the HttpRequest. The IFormFile interface allows us to read the contents of a file via an accessible Stream.

We also call `SanitizeFilename` that runs some simple regex and conditions to validate input.

```c#
private static string SanitizeFilename(string filename)
        {
            var sanitizedFilename = filenameRegex.Replace(filename, "").TrimEnd('.');

            if (sanitizedFilename.Length > MaxFilenameLength)
            {
                sanitizedFilename = sanitizedFilename.Substring(0, MaxFilenameLength);
            }

            return sanitizedFilename;
        }
```



```c#
[HttpPost()]
        public async Task<IActionResult> Upload(IFormFile file)
        {
            var name = SanitizeFilename(file.FileName);

            if (string.IsNullOrWhiteSpace(name))
            {
                throw new ArgumentException();
            }

            using (Stream stream = file.OpenReadStream())
            {
                await storage.Save(stream, name);
            }

            return Accepted();
        }
```



Download - Called when clicking a link of a an image, calls Load method from `BlobStorage` and provides URI of the image.

```c#
[HttpGet("{filename}")]
        public async Task<IActionResult> Download(string filename)
        {
            var stream = await storage.Load(filename);

            // This usage of File() always triggers the browser to perform a file download.
            // We always use "application/octet-stream" as the content type because we don't record
            // any information about content type from the user when they upload a file.
            return File(stream, "application/octet-stream", filename);
        }
```

Can also be demonstrated in the Network tab of DevTools when clicking a link in the browser

<img src="/img/blob1.png">



### Frontend Part

We only have one page `Index`. It uses a CDN delivered script called Dropzone for the upload box. The template uses 2 Javascript functions, one for displaying the blob names along with the images and one for the Dropzone box. 



**FetchLinksAndImages** - Gets the data from our Files controller and appends them to a **`fileList`**, for each URI we also create a new Image object with fixed size and add it to our `imageList`.

```javascript
....
 <h2>Uploaded files</h2>
        <ul id="fileList"></ul>
        <div class="card">
            <div class="card-body">
               <ul id="imageList"</ul>
            </div>
        </div>

....

    // Grab links for files from backend api
    function fetchLinksAndImages() {
        // Fetch links
        $.get("api/Files", function(fetchedLinks) {

            var newList = $('<ul id="fileList" />')
            var newImageList = $('<ul id="imageList" />')

            $.each(fetchedLinks, function(index, value) {
                var li = $('<li/>').appendTo(fileList)
                $('<a/>').attr('href', value).text(value.split('/').pop()).appendTo(li)
                newList.append(li)

                var image = new Image(200,200);
                image.src = value;


                document.getElementById('imageList').appendChild(image);
                newImageList.append(image);

            })

            $('#fileList').replaceWith(newList)
            $('#imageList').replaceWith(newImageList)
        });
    }
```

**Dropzone** - Initiated when uploading a file through a POST form above, calls fetchLinks function when finished to update the site with the new link and image dynamically.

```javascript
....
<div class="box content">
        <hr>
        <h2>Upload files</h2>
        <div>
            <form action="/api/Files"
                  class="dropzone needsclick dz-clickable"
                  id="file-upload"
                  method="post"
                  enctype="multipart/form">

                <div class="dz-message needsclick">
                    <span class="note needsclick">
                        Drop files here or click to upload.
                    </span>
                </div>
            </form>
....
/* Dropzone */
    // "fileUpload" is the camelized version of the HTML element's ID
    Dropzone.options.fileUpload = {
        paramName: "file", // The name that will be used to transfer the file
        dictDefaultMessage: "Drop files here or click to upload",
        addRemoveLinks: true, // Allows for cancellation of file upload and remove thumbnail
        init: function() {
            myDropzone = this;
            myDropzone.on("success", function(file, response) {
                console.log("Success");
                myDropzone.removeFile(file);
                fetchLinksAndImages();
            });
        }
    };
```



## Deployment

To deploy the web app I've added a Dockerfile and a gtihub action workflow that creates a package in my Container Registry.

**DockerFile**

```dockerfile
FROM mcr.microsoft.com/dotnet/aspnet:3.1 AS base
WORKDIR /app
EXPOSE 80
EXPOSE 443

FROM mcr.microsoft.com/dotnet/sdk:3.1 AS build
WORKDIR /src
COPY ["FileUploader.csproj", "."]
RUN dotnet restore "./FileUploader.csproj"
COPY . .
WORKDIR "/src/."
RUN dotnet build "FileUploader.csproj" -c Release -o /app/build

FROM build AS publish
RUN dotnet publish "FileUploader.csproj" -c Release -o /app/publish

FROM base AS final
WORKDIR /app
COPY --from=publish /app/publish .
ENTRYPOINT ["dotnet", "FileUploader.dll"]
```

**WorkFlow**

```yaml
name: Build App & Push to container registry

on: [push]
  
jobs:
    build-and-push-package:
        runs-on: ubuntu-latest
        env: 
          working-directory: .
        steps:
        - name: Checkout code
          uses: actions/checkout@master
          
        - name: Setup .NET ${ { env.DOTNET_VERSION } } Environment
          uses: actions/setup-dotnet@v1
          with:
            dotnet-version: '3.1.x'
            
        - name: Build Project
          run: dotnet build
            
        - name: Login to GitHub Container Registry
          uses: docker/login-action@v1.10.0
          with:
             registry: ghcr.io
             username: ${ { github.actor } };
             password: ${ { secrets.GITHUB_TOKEN } }
             
        - name: Build and push container package
          id: docker_build
          uses: docker/build-push-action@v2.7.0
          with:
            push: true
            context: ${ {env.working-directory} }
            tags: |
              ghcr.io/bjork-dev/blobapp2:latest
              ghcr.io/bjork-dev/blobapp2:${ { github.run_number } }
```

I then created an App Service in Azure and pointed to my Container Registry, and also added the webhook URI to my repository to allow automatic deployment on new package releases.

**Deployment Center Settings**

<img src="/img/azureportal6.png">



## Pricing

Scenario based on an web app with 1000 users that each adds 100MB files to our blob storage everyday and each image is downloaded 3 times a day.

Using Azure Pricing Calculator we can estimate the cost per month with the following settings:

* Region: North Europe
* Type Block-Blob Storage
* Perfomance Level: Standard
* Storage Account: General Usage v2 - Recommended for most scenarios.
* Storage Capacity: 15000 GB - 2883 kr / month 
* Access Level: Hot
* Redundancy: LRS (Local Redundant Storage)
* Write Actions / month: 100000  - 436 kr
* Read Actions / month: 300000 - 104 kr

Total 3429 kr / month.

<img src="/img/price.png">

## Security

Azure implements several security precautions to ensure that our files are safe.

* **Data In Flight** - This describes data transfers over the internet, for example between our app and the cloud storage, or other data movement. To avoid eavesdropping, snooping etc every request must be sent over HTTPS, by default any other request is denied. 

* **Data At Rest** - This decribes data located in the cloud that is not currently being accessed. Azure Storage uses server-side encryption (SSE) to automatically encrypt your data when it is persisted to the cloud. 
* **Azure KeyVault**- We can if we want to, use KeyVault to store our sensitive connectionstrings to our storage account. Since these are basically the key to our perhaps super-sensitive information, Microsoft recommends that you regularly "rotate" these. Meaning that they are switched out with new strings. KeyVault can automate this process so you as a developer won't even have to remember to click the rotate button. This ensures that even if someone were to get access to our connection string, it wont be usable once we've rotated. 
* **SAS** - As described in the beginning of the post, if we want to allow for third-party support (like an external service to upload and download to our container) we could implement SAS (Shared Access Signature). This elimantes the need to provide sensitive information, like a connection string to a third-party. Instead they are granted temporary access keys to a specific resource that the administrator decides, we can even go as far as choosing specific IP-addresses that are allowed to connect with the SAS token / URL.



##### Sources

##### [Store data in Azure](https://docs.microsoft.com/en-us/learn/paths/store-data-in-azure/)

##### [Azure Data Encryption](https://cloudacademy.com/blog/how-does-azure-encrypt-data/)

##### [Azure SAS](https://docs.microsoft.com/sv-se/azure/storage/common/storage-sas-overview)

##### [Azure Storage Accounts Overview](https://docs.microsoft.com/en-us/azure/storage/common/storage-account-overview?toc=/azure/storage/blobs/toc.json)
